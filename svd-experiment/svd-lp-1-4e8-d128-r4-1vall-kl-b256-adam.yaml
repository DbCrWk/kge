job:
  type: train
  device: 'cuda:0'
random_seed:
  python: -1
  torch: -1
  numpy: -1

svd:
  rank: 4
  entity_embedder:
    dim: 128
  relation_embedder:
    regularize: 'lp'
    regularize_args:
      p: 1
      weight: 4e-8

dataset:
  name: 'fb15k-237'
  num_entities: -1
  num_relations: -1
  files:
    train:
      filename: train.del
      type: triples
    valid:
      filename: valid.del
      type: triples
    test:
      filename: test.del
      type: triples
    entity_ids:
      filename: entity_ids.del
      type: map
    relation_ids:
      filename: relation_ids.del
      type: map
    entity_strings:
      filename: entity_ids.del  # default to id file
      type: map
    relation_strings:
      filename: relation_ids.del  # default to id file
      type: map
    +++: +++
  pickle: True
  +++: +++

model: 'svd'

train:
  split: train
  type: 1vsAll
  loss: kl
  loss_arg: .nan
  max_epochs: 400
  batch_size: 256
  # Number of workers used to construct batches. Leave at 0 for default.
  num_workers: 0
  optimizer: Adam           # sgd, adagrad, adam
  optimizer_args:
    +++: +++
  lr_scheduler: ""
  lr_scheduler_args:
    +++: +++
  trace_level: epoch           # batch, epoch
  checkpoint:
    every: 5
    keep: 3
  auto_correct: False
  abort_on_nan: True
  visualize_graph: False
  pin_memory: False

KvsAll:
  label_smoothing: 0.0
  query_types:
    sp_: True
    s_o: False
    _po: True

negative_sampling:
  sampling_type: uniform
  frequency:
    smoothing: 1
  num_samples:
    s: 3
    p: 0          # -1 means: same as s
    o: -1         # -1 means: same as s
  filtering:
    s: False       # filter and resample for slot s
    p: False       # as above
    o: False       # as above
    split: ''      # split containing the positives; default is train.split
    implementation: fast_if_available
  shared: False
  with_replacement: True
  implementation: triple
  chunk_size: -1                  # default: no chunking

eval:
  split: valid
  filter_splits: [ 'train', 'valid' ]
  filter_with_test: True
  type: entity_ranking
  hits_at_k_s: [1, 3, 10, 50, 100, 200, 300, 400, 500, 1000]
  batch_size: 512
  chunk_size: -1                  # default: no chunking
  metrics_per:
    relation_type: True          # 1-to-1, 1-to-N, N-to-1, N-to-N
    head_and_tail: False          # head, tail
    argument_frequency: False     # 25%, 50%, 75%, top quantiles per argument
  trace_level: epoch            # example, batch, epoch
  num_workers: 0
  pin_memory: False

valid:
  split: 'valid'
  every: 2
  metric: mean_reciprocal_rank_filtered_with_test
  metric_expr: 'float("nan")'
  early_stopping:
    patience: 10
    min_threshold:
      epochs: 50
      metric_value: 0.05
  trace_level: epoch
